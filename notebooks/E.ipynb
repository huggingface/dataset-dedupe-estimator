{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8ea07ac",
   "metadata": {},
   "source": [
    "# Parquet Content-Defined Chunking\n",
    "\n",
    "Apache Parquet is a columnar storage format that is widely used in the data engineering community. \n",
    "\n",
    "As Hugging Face hosts nearly 11PB of datasets with Parquet files alone accounting for over 2.2PB of that storage, optimizing Parquet storage is of high priority.\n",
    "Hugging Face has introduced a new storage layer called [Xet](https://huggingface.co/blog/xet-on-the-hub) that leverages content-defined chunking to efficiently deduplicate chunks of data reducing storage costs and improving download/upload speeds.\n",
    "\n",
    "While Xet is format agnostic, Parquet's layout and column-chunk (data page) based compression can produce entirely different byte-level representations for data with minor changes, leading to suboptimal deduplication performance. To address this, the Parquet files should be written in a way that minimizes the byte-level differences between similar data, which is where content-defined chunking (CDC) comes into play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "648605ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import wikipedia\n",
    "from fastcdc import fastcdc\n",
    "from dataclasses import dataclass\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Chunk:\n",
    "    \"\"\"Represents a single chunk of data.\"\"\"\n",
    "    hash: str\n",
    "    data: bytes\n",
    "    first_seen_in: int = 0\n",
    "\n",
    "\n",
    "class Store(dict):\n",
    "    \"\"\"\n",
    "    Simple in-memory content-addressable store for CDC chunks.\n",
    "    \"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def from_bytes(cls, data: bytes):\n",
    "        \"\"\"\n",
    "        Apply FastCDC to the input data and create a map of hashed chunks.\n",
    "        \"\"\"\n",
    "        chunks = fastcdc(\n",
    "            data,\n",
    "            fat=True, \n",
    "            hf=hashlib.sha256, \n",
    "            min_size=64, \n",
    "            avg_size=256, \n",
    "            max_size=1024\n",
    "        )\n",
    "        store = cls()\n",
    "        for chunk in chunks:\n",
    "            store[chunk.hash] = Chunk(hash=chunk.hash, data=chunk.data)\n",
    "        return store\n",
    "    \n",
    "    @classmethod\n",
    "    def merge(cls, stores):\n",
    "        \"\"\"\n",
    "        Merge multiple chunk stores into a single store while updating \n",
    "        each chunk's first_seen_in attribute for later visualization.\n",
    "        \"\"\"\n",
    "        merged = cls()\n",
    "        for index, store in enumerate(stores):\n",
    "            for chunk in store.chunks:\n",
    "                if chunk.hash not in merged:\n",
    "                    merged[chunk.hash] = Chunk(\n",
    "                        hash=chunk.hash,\n",
    "                        data=chunk.data,\n",
    "                        first_seen_in=index\n",
    "                    )\n",
    "        return merged\n",
    "    \n",
    "    @property\n",
    "    def chunks(self):\n",
    "        return list(self.values())\n",
    "    \n",
    "    @property\n",
    "    def num_chunks(self):\n",
    "        return len(self)\n",
    "    \n",
    "    @property\n",
    "    def overall_size(self):\n",
    "        return sum(len(chunk.data) for chunk in self.chunks)\n",
    "\n",
    "    def visualize(self):\n",
    "        \"\"\"\n",
    "        Visualize the store as a colored strip in ipython.\n",
    "        Each chunk's width is proportional to its size.\n",
    "        \"\"\"\n",
    "        colors = ['#00FF00', '#FF0000', '#0000FF', '#FFFF00', '#FF00FF', '#00FFFF']\n",
    "        html = '<div style=\"display: flex; flex-direction: row; width: 100%;\">'\n",
    "        total_size = self.overall_size\n",
    "        for chunk in self.chunks:\n",
    "            color = colors[chunk.first_seen_in % len(colors)]\n",
    "            width_percent = 100 * len(chunk.data) / total_size if total_size else 0\n",
    "            html += f'<div style=\"background-color: {color}; height: 20px; width: {width_percent}%;\"></div>'\n",
    "        html += '</div>'\n",
    "        html += '<p>Overall size: {} bytes</p>'.format(self.overall_size)\n",
    "        html += '<p>Number of chunks: {}</p>'.format(self.num_chunks)\n",
    "        display(HTML(html))\n",
    "\n",
    "    \n",
    "def cdc(data):\n",
    "    \"\"\"Store data in a content-addressable store using content-defined chunking.\"\"\"\n",
    "    if isinstance(data, bytes):\n",
    "        return Store.from_bytes(data)\n",
    "    elif isinstance(data, str):\n",
    "        data = data.encode('utf-8')\n",
    "        return Store.from_bytes(data)\n",
    "    elif isinstance(data, list):\n",
    "        return Store.merge([cdc(item) for item in data])\n",
    "    else:\n",
    "        raise TypeError(\"Unsupported data type for chunking: {}\".format(type(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "bbb24d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "article = wikipedia.page(\"Apache Arrow\").content\n",
    "edited_article = \"Apache Arrow is a very cool project!\\n\" + article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "e2a40c96",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Store' object has no attribute 'summary'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[150]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mcdc\u001b[49m\u001b[43m(\u001b[49m\u001b[43medited_article\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msummary\u001b[49m()\n",
      "\u001b[31mAttributeError\u001b[39m: 'Store' object has no attribute 'summary'"
     ]
    }
   ],
   "source": [
    "cdc(edited_article).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "adb79a40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"display: flex; flex-direction: row; width: 100%;\"><div style=\"background-color: #00FF00; height: 20px; width: 5.162907268170426%;\"></div><div style=\"background-color: #00FF00; height: 20px; width: 7.719298245614035%;\"></div><div style=\"background-color: #00FF00; height: 20px; width: 8.220551378446116%;\"></div><div style=\"background-color: #00FF00; height: 20px; width: 13.082706766917294%;\"></div><div style=\"background-color: #00FF00; height: 20px; width: 6.015037593984962%;\"></div><div style=\"background-color: #00FF00; height: 20px; width: 8.62155388471178%;\"></div><div style=\"background-color: #00FF00; height: 20px; width: 13.18295739348371%;\"></div><div style=\"background-color: #00FF00; height: 20px; width: 9.824561403508772%;\"></div><div style=\"background-color: #00FF00; height: 20px; width: 5.4636591478696745%;\"></div><div style=\"background-color: #00FF00; height: 20px; width: 4.81203007518797%;\"></div><div style=\"background-color: #00FF00; height: 20px; width: 8.922305764411028%;\"></div><div style=\"background-color: #00FF00; height: 20px; width: 1.9548872180451127%;\"></div><div style=\"background-color: #FF0000; height: 20px; width: 7.017543859649122%;\"></div></div><p>Overall size: 1995 bytes</p><p>Number of chunks: 13</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cdc([article, edited_article, article]).visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "c55d82fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Chunk(hash='34a17202faaef45473b7846d5964909e3008e7ee91667a15da7344d7b3b78e5b', data=b'Apache Arrow is a very cool project!\\nApache Arrow is a language-agnostic software framework for developing data analytics applications that ', first_seen_in=0),\n",
       " Chunk(hash='57f2fb4d30543d0c53b07398e8a2733a0656fad4de122ac8940d59f5c7edf40d', data=b'process columnar data. It contains a standardized column-oriented memory format that is able to represent flat and hierarchical data for efficient analyti', first_seen_in=0),\n",
       " Chunk(hash='f2e85785416f5787b77b8b7c70c1096c3511e752118fd81f39bcc0f701d80524', data=b'c operations on modern CPU and GPU hardware. This reduces or eliminates factors that limit the feasibility of working with large sets of data, such as the cost, vol', first_seen_in=0)]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cdc(edited_article).chunks[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "09ed0585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'03e91020d386b54235013ed9112eb4906ff1f13e1c4cbb0fc5ddcfc161cf7c40': 'Apache Arrow is a language-agnostic software framework for developing data analytics applications that ',\n",
       " '57f2fb4d30543d0c53b07398e8a2733a0656fad4de122ac8940d59f5c7edf40d': 'process columnar data. It contains a standardized column-oriented memory format that is able to represent flat and hierarchical data for efficient analyti',\n",
       " 'f2e85785416f5787b77b8b7c70c1096c3511e752118fd81f39bcc0f701d80524': 'c operations on modern CPU and GPU hardware. This reduces or eliminates factors that limit the feasibility of working with large sets of data, such as the cost, vol',\n",
       " '8e70c62081aeb4ead6d4d501418182ce4fa84ec6101eeadf9ab4e1519c102f86': 'atility, or physical constraints of dynamic random-access memory.\\n\\n\\n== Interoperability ==\\nArrow can be used with Apache Parquet, Apache Spark, NumPy, PySpark, pandas and other data processing libraries.\\nThe project includes native software libraries written in',\n",
       " 'e593034ec59e9b2beae47fc690f115299cb183ef89744e76039bc238ec525f6f': ' C, C++, C#, Go, Java, JavaScript, Julia, MATLAB, Python (PyArrow), R, Ruby, and Rust. Arrow allows for zero-copy reads ',\n",
       " '5c18c934d998702b33fe1dc5e69f48a08cae8b0316ed38b3cf46cde23b090d69': 'and fast data access and interchange without serialization overhead between these languages and systems.\\n\\n\\n== Applications ==\\nArrow has been used in diverse domains, includ',\n",
       " '588c02f7355c1e5649f4e66ea2ca069b9c98b46898f84afa81aba5eb2cf404c0': 'ing analytics, genomics, and cloud computing.\\n\\n\\n=== Comparison to Apache Parquet and ORC ===\\nApache Parquet and Apache ORC are popular examples of on-disk columnar data formats. Arrow is designed as a complement to these formats for processing data in-memory. The',\n",
       " 'a24b7ffceb99457cb94c803788fa105501da576eb112d802e581bfa001e41c0d': ' hardware resource engineering trade-offs for in-memory processing vary from those associated with on-disk storage. The Arrow and Parquet projects include libraries that allow for reading and writ',\n",
       " 'db7435e985212984e4672e830bfae673ed0a8cd85131bcbea58b9a6a781b5e73': 'ing data between the two formats.\\n\\n\\n== Governance ==\\nApache Arrow was announced by The Apache Software Founda',\n",
       " '51fa9e305787224c9d8de01641fb57ff7099df4088240d8ddda7148a85c9a21e': 'tion on February 17, 2016, with development led by a coalition of developers from other open sou',\n",
       " '9219e67df382e4107bbb6d8d2cd3f54ff1e15a69f1ccae17186d92834aca8932': 'rce data analytics projects.  The initial codebase and Java library was seeded by code from Apache Drill.\\n\\n\\n== References ==\\n\\n\\n== External links ==\\nApache Arrow project web site\\n',\n",
       " '2d15b1ecef6160702a98466a918229ca9f2d3b2b908672066e8cae3addec3d18': 'Apache Arrow GitHub project source code',\n",
       " '34a17202faaef45473b7846d5964909e3008e7ee91667a15da7344d7b3b78e5b': 'Apache Arrow is a very cool project!\\nApache Arrow is a language-agnostic software framework for developing data analytics applications that '}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cdc([article, edited_article])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "24e53ddf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'7f8cf0933e7e94188a5524021cbc02c1a3f7da933fb7ef33a079cd452bb874d3': 'Apache Parquet\\nApache Parquet is a free and open-source column-oriented data storage format in the Apache Hadoop ecosystem. It is similar to RCFile and ORC, the other columnar-storage file formats in Hadoop, and is compatible with most of the data processing frameworks around Hadoo',\n",
       " '5e1be5561bd23e3ef658c5a5941a1d70749c74e2bb7b78310cf81a86fa35cd81': 'p. It provides efficient data compression and encoding schemes with enhanced performance to handle complex data in bulk.\\n\\n\\n== History ==\\nThe open-source project to build Apache Parquet began as a joint effort between Twitter and Cloudera. Parquet was designed as an improvement ',\n",
       " '68fe3ba35cdcbe968c3660174e4678846f3576e312f83b2b3b78cc6374d16b8f': 'on the Trevni columnar storage format created by Doug Cutting, the creator of Hadoop. The first version, Apache Parquet 1.0, was released in July 2013. Since April 27, 2015, Apache Parquet has been a top-level Apache Software Foundation (A',\n",
       " 'b84ad7bd9db80d5021faf3226b6dcb8b6cc8f0c13a2224939f69e58bba6fc9fa': 'SF)-sponsored project.\\n\\n\\n== Features ==\\nApache Parquet is implemented using the record-shredding and assembly algorithm, which accommodates the complex data structures that can be used to store da',\n",
       " 'ed5b19eb85c5bcc510326add1f9f7bbdceba52779ee09c4e80b399b8eddfca0a': 'ta. The values in each column are stored in contiguous memory locations, providing the following benefits:\\n\\nColumn-wise compression is efficient in storage space\\nEncoding and compression techniques specific to the type of data in each column can be used\\nQueries that ',\n",
       " 'a4ce2a41978c30b218714b994f021ef79cd3ea033120f0290732a74ea108f0a1': 'fetch specific column values need not read the entire row, thus improving performance\\nApache Parquet i',\n",
       " 'f7b05d46529b628fde12ed1026dd78917cbd509853046cb8ac4412360b6b0703': 's implemented using the Apache Thrift framework, which increases its flexibility; it can work with a number of programming la',\n",
       " 'e5737a0899030fb50a308dc17fa4f07820f4ed7b5c460d354ac4f38194d6bf1a': 'nguages like C++, Java, Python, PHP, etc.\\nAs of August 2015, Parquet supports the big-data-processing frameworks including Apache Hive, Apache Drill, Apach',\n",
       " '6d2d0120ce35efd5246c2607b09a6cefc325b2f0e7edcfbfd7a3be394d0a65e2': 'e Impala, Apache Crunch, Apache Pig, Cascading, Presto and Apache Spark. It is one of the external data formats used by the pandas Python data manipulation and analysis library.\\n\\n\\n== Compression and encoding ==\\nIn P',\n",
       " '737be6364e115adb3d3273c0dbc403b1f0b21840cdde2dd93e6490dc70fb354c': 'arquet, compression is performed column by column, which enables different encoding schemes to be used for text and integer data. This strategy also keeps the door open for newer and better encoding schemes to be implemented as they are invented.\\nParquet supports various compression formats: snappy, gzip, LZO, brotli, zstd, a',\n",
       " '132c9e09e260e41593f46791f98f323870b470b5e0aec53ce97250fc8d91ab94': 'nd LZ4.\\n\\n\\n=== Dictionary encoding ===\\nParquet has an automatic dictionary encoding enabled dynamically for data with a small number of unique values (i.e. below 105) that enables significant compression and boEdited Contentosts pro',\n",
       " '4332334b32e7cccad653422c3b8d5fa99f282c0dcd417cba3223c1f67fe263e2': 'cessing speed.\\n\\n\\n=== Bit packing ===\\nStorage of integers is usually done with dedicated 32 or 64 bits per integer. For small integers, packing multiple integers into the same space makes storage more efficient.\\n\\n\\n=== Run-length encoding (RLE) ===\\nTo optimize storage of multiple occurrences of the same value, run-length encoding is used, which is where a single value is stored once along with the number of occurrences.\\nParquet implem',\n",
       " '201e06d18fbbad47941cccfe04c571f90f08e1ab519be1968a65fbb64df90956': 'ents a hybrid of bit packing and RLE, in which the encoding switches based on which produces the best compression results. This strategy works well for ce',\n",
       " '86f78d9da53602b3949a13399a686dc48318644c7f1c46e2488f128ee383434f': 'rtain types of integer data and combines well with dictionary encoding.\\n\\n\\n== Cloud Storage and Data Lakes ==\\nParquet is widely used as the underlying file format in modern cloud-based da',\n",
       " '18f74942d522d7d9da0bcdcae8bf46f267205cbb0633e88189cfcaa11cd6abbb': 'ta lake architectures. Cloud storage systems such as Amazon S3, Azure Data Lake Storage, and Google Cloud Storage commonly store data in Parquet format due to its efficient columnar representation and retrieval capabilities. Data lakehouse frameworks—including Apache Iceberg, Delta Lake, and Apache Hudi —build an additional metadata layer on top of Parquet files to support features such as schema evolution, time-travel queries, and ACID-compliant transac',\n",
       " 'a295ae1fa6f50d8f0fb464fe5e52b9bb8e92f33396eea7ab83f5e339b8551272': 'tions. In these architectures, Parquet files serve as the immutable storage layer while the table formats manage data versioning and transactional integrity.\\n\\n\\n== Comparison ==\\nApache Parquet is comparable to RCFile and Optimized Row Columnar (ORC) file formats — all three fall under the category of columnar data storage within the Hadoop ecosystem. They all have better compression and encoding with improved read performance at the cost ',\n",
       " '86e046201ba2f45aab77daa4e95d741355b0aa1125094aac5c9166fca40f5815': 'of slower writes. In addition to these features, Apache Parquet supports limited schema evolution, i.e., the schema can be modified according to the changes in the dat',\n",
       " '144e1dd6bc063163f7c15d14dde7713e6f08ed6068676b400aa647c3f1956636': 'a. It also provides the ability to add new columns and merge schemas that do not conflict.\\nApache Arrow is designed as an in-memory complement to on-disk columnar formats like Parquet and ORC. ',\n",
       " '4a17fac32164a2c52f9aa2d9b28af3576a4d43e976bdb7135489c0ac6d094f98': 'The Arrow and Parquet projects include libraries that allow for reading and writing between the two formats.\\n\\n\\n== Implementations ==\\nKnown implementations of Parquet include:\\n\\n\\n== See also ==\\n\\n\\n== References ==\\n\\n\\n',\n",
       " 'e3fb7154f76a1ca084237857c3c6bda29cc01ac6a40f8e9e02960c947bfb776d': '== External links ==\\nOfficial website \\nThe Parquet Format and Performance Optimization Opportunities on YouTube\\nDremel paper'}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk([edited_article])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f1272d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Store(num_chunks=19, overall_size=4760 bytes)\n"
     ]
    }
   ],
   "source": [
    "print(chunk([article, article]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4a9001c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5709"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk([article, edited_article]).overall_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "75fce7f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk([article, article]).num_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "86dbbc0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'4e27765a44b02d50897d0dad6710bcae3e0aa1ca32fb55c3478aa73d56e14756',\n",
       " '52c4d8a7cc5549d7f1dd22102e26a410993fab646a40e16bcf3d3256cb018e56'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_chunks.keys() - edited_article_chunks.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4998825d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'132c9e09e260e41593f46791f98f323870b470b5e0aec53ce97250fc8d91ab94',\n",
       " '4332334b32e7cccad653422c3b8d5fa99f282c0dcd417cba3223c1f67fe263e2',\n",
       " '7f8cf0933e7e94188a5524021cbc02c1a3f7da933fb7ef33a079cd452bb874d3'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edited_article_chunks.keys() - article_chunks.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444ef669",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Content-Defined Chunking (CDC)\n",
    "\n",
    "TODO: briefly explain what CDC is and how it works through an example, show a negative example on a parquet file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bfc7ac",
   "metadata": {},
   "source": [
    "\n",
    "To address this, we introduced a new feature in the Apache Parquet C++ implementation that allows users to write Parquet files in a way that minimizes the number changed data pages \n",
    "\n",
    "Content-defined chunking (CDC) is a technique that divides data into variable-sized chunks based on the content of the data itself, rather than fixed-size blocks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b769bc8-3d10-4f10-98f0-457da30d6a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import glob\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.compute as pc\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "from de import estimate\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0ce3d57-e227-44e9-b773-19b4e08b9ae0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb27103d02a94d19a57343d316b885cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c46f0b4d34634adeb062945f38b78002",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = load_dataset(\"wikimedia/wikipedia\", \"20231101.en\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c943de1-b9af-4011-9c85-4613fe8b2baa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d157830b759b438da80e344528bfeda8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/7 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "20200062385"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.to_parquet(\"wiki.parquet\", use_content_defined_chunking=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0019c984-f254-49b6-81ae-c79dc2beed23",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pq.read_table(\"wiki.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b2bb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = table[:100_000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aaeebd08-6aa2-4fb5-b283-cd76cd9e1f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6407814\n"
     ]
    }
   ],
   "source": [
    "print(len(table))\n",
    "table = table[:100_000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "04ee6486-e874-4dc3-9e31-e39181f25748",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dataset(table, base_dir, num_shards, **kwargs):\n",
    "    # ensure that directory exists\n",
    "    base_dir = pathlib.Path(base_dir)\n",
    "    base_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    rows_per_file = len(table) / num_shards\n",
    "    for i in range(num_shards):\n",
    "        start = i * rows_per_file\n",
    "        end = min((i + 1) * rows_per_file, len(table))\n",
    "        shard = table.slice(start, end - start)\n",
    "        path = base_dir / f\"part-{i}.parquet\"\n",
    "        pq.write_table(shard, path, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea6e098",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_deduplication_ratio(*patterns):\n",
    "    # find all parquet files matching the patterns\n",
    "    paths = sum([glob.glob(pattern) for pattern in patterns], [])\n",
    "    total_size, deduped_size, compressed_deduped_size = estimate(paths)\n",
    "    print(f\"Total size: {total_size / (1024 * 1024):.2f} MB\")\n",
    "    print(f\"Deduped size: {deduped_size / (1024 * 1024):.2f} MB\")\n",
    "    print(f\"Compressed deduped size: {compressed_deduped_size / (1024 * 1024):.2f}\")\n",
    "    return deduped_size / total_size\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2aa9cd08-db26-46bd-a597-da49071dac92",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dataset(table, \"shard1\", num_shards=1, use_content_defined_chunking=False)\n",
    "write_dataset(table, \"shard4\", num_shards=4, use_content_defined_chunking=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "697c3385",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dataset(table, \"shard1\", num_shards=1, use_content_defined_chunking=True)\n",
    "write_dataset(table, \"shard4\", num_shards=4, use_content_defined_chunking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5af6bf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dataset(table, \"shard20\", num_shards=20, use_content_defined_chunking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "19f1c230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size: 536.72 MB\n",
      "Deduped size: 275.84 MB\n",
      "Compressed deduped size: 276.64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.515424550609722"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimate_deduplication_ratio(\"shard1/*.parquet\", \"shard4/*.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d854e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['shard8/part-2.parquet', 'shard8/part-3.parquet', 'shard8/part-1.parquet', 'shard8/part-0.parquet', 'shard8/part-5.parquet', 'shard8/part-4.parquet', 'shard8/part-6.parquet', 'shard8/part-7.parquet', 'shard20/part-18.parquet', 'shard20/part-2.parquet', 'shard20/part-11.parquet', 'shard20/part-10.parquet', 'shard20/part-3.parquet', 'shard20/part-19.parquet', 'shard20/part-8.parquet', 'shard20/part-12.parquet', 'shard20/part-1.parquet', 'shard20/part-0.parquet', 'shard20/part-13.parquet', 'shard20/part-9.parquet', 'shard20/part-16.parquet', 'shard20/part-5.parquet', 'shard20/part-4.parquet', 'shard20/part-17.parquet', 'shard20/part-6.parquet', 'shard20/part-15.parquet', 'shard20/part-14.parquet', 'shard20/part-7.parquet']\n",
      "Total size: 22203.23 MB\n",
      "Deduped size: 11201.79 MB\n",
      "Compressed deduped size: 11233.67\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5059472759226442"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimate_deduplication_ratio(\"shard8/*.parquet\", \"shard20/*.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48dcf0a1-a657-4a2f-aea1-e6df4988d794",
   "metadata": {},
   "outputs": [],
   "source": [
    "mini = table[:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51740a73-e17b-4042-8d0f-d40a372a4fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pq.write_table(mini, \"mini.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0179d9e4-036f-4f81-b6b3-84bd4c66a7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pq.write_table(mini[:50_000], \"nocdc1.parquet\", use_content_defined_chunking=False)\n",
    "pq.write_table(mini[50_000:], \"nocdc2.parquet\", use_content_defined_chunking=False)\n",
    "pq.write_table(mini[:50_000], \"cdc1.parquet\", use_content_defined_chunking=True)\n",
    "pq.write_table(mini[50_000:], \"cdc2.parquet\", use_content_defined_chunking=True)\n",
    "pq.write_table(mini, \"cdc.parquet\", use_content_defined_chunking=True)\n",
    "pq.write_table(mini, \"nocdc.parquet\", use_content_defined_chunking=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "178d4392-fdbd-4962-b746-8dfc9ad6a282",
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_ = pa.concat_tables([mini[30_000:], mini[:30_000]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5f0aaf28-4016-4796-884d-856102e7f3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pq.write_table(mini_, \"cdc_.parquet\", use_content_defined_chunking=True)\n",
    "pq.write_table(mini_, \"nocdc_.parquet\", use_content_defined_chunking=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6923d3a0-9b04-4554-a5b3-c6dc85fc40d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pq.write_table(mini_, \"cdc__.parquet\", use_content_defined_chunking=True, row_group_size=20_000)\n",
    "pq.write_table(mini_, \"nocdc__.parquet\", use_content_defined_chunking=False, row_group_size=20_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "441f2789-83fd-452b-8db7-a7e55beeb6ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f28274a811b4defa865bcbb6edeb8fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ea421a475624b4d9e91f775f6fed396",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = load_dataset(\"HuggingFaceTB/finemath\", \"finemath-4plus\", split=\"train[:1_000_000]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa25f4ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6cfefa80a3740d7a80b6addf09436f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "5822002066"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.to_parquet(\"finemath.parquet\", use_content_defined_chunking=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc1b4252",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pq.read_table(\"finemath.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83ea9839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--  1 kszucs  staff   2.6G Jul  7 11:03 finemath.parquet\n"
     ]
    }
   ],
   "source": [
    "!ls -lh finemath.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2de3a81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e986d903",
   "metadata": {},
   "source": [
    "### Add a new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9c9afe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68c92a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arrow13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
